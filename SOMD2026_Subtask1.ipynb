{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4ac3df3d",
      "metadata": {},
      "source": [
        "1. Imports + Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fe88c28",
      "metadata": {},
      "source": [
        "- Import all required libraries.\n",
        "- Define file paths for the train/test data and labels.\n",
        "- Implement small helper functions.\n",
        "- Load the raw JSONL data into Pandas dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c095451c",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.preprocessing import normalize\n",
        "import faiss\n",
        "import hdbscan\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths to the shared task data\n",
        "TRAIN_DATA_PATH = \"train_data.jsonl\"\n",
        "TEST_DATA_PATH = \"test_data.jsonl\"\n",
        "TRAIN_LABELS_PATH = \"rain_labels.json\"\n",
        "\n",
        "\n",
        "def load_jsonl(path: str):\n",
        "    \"\"\"Load a JSONL file into a list of dicts.\"\"\"\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        return [json.loads(line) for line in f]\n",
        "\n",
        "\n",
        "def create_context(row: pd.Series) -> str:\n",
        "    \"\"\"Build a simple textual context for a mention from its fields and relations.\"\"\"\n",
        "    mention = str(row.get('mention', '')).lower()\n",
        "    # Duplicate the mention to give it extra weight, then add type and related mentions\n",
        "    parts = [mention, mention, str(row.get('type', ''))]\n",
        "    for rel in row.get('relations', []):\n",
        "        parts.append(str(rel.get('mention', '')))\n",
        "    return \" \".join(parts)\n",
        "\n",
        "\n",
        "def clean_name(name: str) -> str:\n",
        "    \"\"\"Normalize a surface form: lowercase and remove non-alphanumeric chars.\"\"\"\n",
        "    if not isinstance(name, str):\n",
        "        return \"\"\n",
        "    return re.sub(r'[^a-z0-9]', '', name.lower())\n",
        "\n",
        "\n",
        "# Load sentence-transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Load data\n",
        "train_data = load_jsonl(TRAIN_DATA_PATH)\n",
        "test_data = load_jsonl(TEST_DATA_PATH)\n",
        "\n",
        "# Load clustering for the train set\n",
        "with open(TRAIN_DATA_PATH.replace('train_data.jsonl', 'train_labels.json'), 'r') as f:\n",
        "    train_labels = json.load(f)\n",
        "\n",
        "# Convert to data frame\n",
        "df_train = pd.DataFrame(train_data)\n",
        "df_test = pd.DataFrame(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "606e8f7c",
      "metadata": {},
      "source": [
        "2. Centroid Representation\n",
        "\n",
        "- Build a textual context for each training mention.\n",
        "- Encode all train contexts with the sentence-transformer model.\n",
        "- For each gold cluster (from `train_labels`), average member embeddings to get a centroid.\n",
        "- Collect these centroids into an array of knowledge-base (KB) vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c03fb97",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build context strings\n",
        "df_train['context'] = df_train.apply(create_context, axis=1)\n",
        "\n",
        "# Encode train mention contexts\n",
        "train_embs = normalize(model.encode(df_train['context'].tolist(), show_progress_bar=True))\n",
        "# Map mention_id to embedding vector\n",
        "id_to_emb = dict(zip(df_train['mention_id'], train_embs))\n",
        "\n",
        "# Compute one centroid  per train cluster\n",
        "kb_vectors = []\n",
        "for group in train_labels:\n",
        "    # Keep only mention_ids that we have embeddings for\n",
        "    group_embs = [id_to_emb[m_id] for m_id in group if m_id in id_to_emb]\n",
        "    if group_embs:\n",
        "        centroid = np.mean(group_embs, axis=0)\n",
        "        # Re-normalize centroid to unit length\n",
        "        kb_vectors.append(centroid / np.linalg.norm(centroid))\n",
        "\n",
        "kb_vectors = np.array(kb_vectors)\n",
        "print(f\"Gold standard {len(kb_vectors)} clusters created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3c3fd2b",
      "metadata": {},
      "source": [
        "3. Matching test mentions to KB centroids with FAISS\n",
        "\n",
        "- Build context strings and embeddings for **test** mentions.\n",
        "- Index all KB centroids in a FAISS inner-product index.\n",
        "- For each test mention, retrieve its nearest KB centroid and score.\n",
        "- Combine semantic similarity (embedding score) with exact name matching to decide whether to attach to an existing KB cluster or mark as unmatched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d2359397",
      "metadata": {},
      "outputs": [],
      "source": [
        "# create embeddings for test mentions\n",
        "df_test['context'] = df_test.apply(create_context, axis=1)\n",
        "test_embs = normalize(model.encode(df_test['context'].tolist(), show_progress_bar=True))\n",
        "\n",
        "# Similarity threshold \n",
        "THRESHOLD = 0.7\n",
        "\n",
        "# Build FAISS index over centroids (cosine similarity via inner product on normalized vectors)\n",
        "dim = kb_vectors.shape[1]\n",
        "index = faiss.IndexFlatIP(dim)\n",
        "index.add(kb_vectors.astype('float32'))\n",
        "\n",
        "print(f\"Searching KB index for {len(test_embs)} mentions\")\n",
        "# For each test embedding, get top centroid and similarity score\n",
        "scores, indices = index.search(test_embs.astype('float32'), 1)\n",
        "\n",
        "best_match_indices = indices.flatten()\n",
        "max_scores = scores.flatten()\n",
        "\n",
        "# assignments[i] = chosen KB cluster index or -1 if we create a new cluster later\n",
        "assignments = []\n",
        "# unmatched_indices = positions in the test array that do not attach to any KB cluster\n",
        "unmatched_indices = []\n",
        "\n",
        "# Precompute exact name -> KB cluster mapping from train data\n",
        "id_to_text_map = {row['mention_id']: row['mention'] for row in train_data}\n",
        "kb_exact_names = {}\n",
        "for group_idx, m_ids in enumerate(train_labels):\n",
        "    for m_id in m_ids:\n",
        "        name = id_to_text_map.get(m_id, \"\").lower()\n",
        "        if name:\n",
        "            kb_exact_names[name] = group_idx\n",
        "\n",
        "# Lowercased surface forms for test mentions\n",
        "test_mentions = df_test['mention'].str.lower().values\n",
        "\n",
        "for i in range(len(test_embs)):\n",
        "    max_score = max_scores[i]\n",
        "    best_idx = best_match_indices[i]\n",
        "    current_text = test_mentions[i]\n",
        "\n",
        "    # High semantic similarity: directly attach to best KB cluster\n",
        "    if max_score >= THRESHOLD:\n",
        "        assignments.append(best_idx)\n",
        "    # Medium similarity: allow a match if exact surface form exists in the KB\n",
        "    elif max_score > 0.50 and current_text in kb_exact_names:\n",
        "        assignments.append(kb_exact_names[current_text])\n",
        "    # Very low similarity or unseen name: mark as unmatched to be clustered later\n",
        "    else:\n",
        "        assignments.append(-1)\n",
        "        unmatched_indices.append(i)\n",
        "\n",
        "print(f\"Matched {len(assignments) - len(unmatched_indices)} mentions to existing clusters.\")\n",
        "print(f\"{len(unmatched_indices)} mentions need new clusters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "968c8667",
      "metadata": {},
      "source": [
        "4. Create extra clusters from remaining mentions \n",
        "\n",
        "- Normalize mention strings and build an abbreviation map (short form â†’ long form).\n",
        "- Derive a canonical name for each test mention.\n",
        "- Run HDBSCAN on **only the unmatched** mentions, grouped by entity type (and optionally by first letter) to keep blocks manageable.\n",
        "- Merge KB assignments and HDBSCAN clusters into a single set of final clusters and write them to `test_labels.json`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b0945f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------\n",
        "# Step 1: Normalize names and build abbreviation map\n",
        "# -------------------------------------------------------------\n",
        "print(\"Mapping abbreviations...\")\n",
        "df_test['clean_name'] = df_test['mention'].str.lower().str.replace(r'[^a-z0-9]', '', regex=True)\n",
        "\n",
        "abbrev_map = {}\n",
        "rel_mask = df_test['relations'].map(len) > 0\n",
        "# For mentions that have relations, collect all Abbreviation relations\n",
        "for row in tqdm(df_test[rel_mask][['clean_name', 'relations']].to_dict('records'), desc=\"Mapping Abbrevs\"):\n",
        "    long_n = row['clean_name']\n",
        "    for rel in row['relations']:\n",
        "        if rel.get('type') == 'Abbreviation':\n",
        "            short_n = clean_name(rel.get('mention', '')) \n",
        "            if long_n and short_n:\n",
        "                abbrev_map[short_n] = long_n\n",
        "\n",
        "# Canonical name prefers the long form if we know an abbreviation mapping\n",
        "df_test['canonical_name'] = df_test['clean_name'].map(abbrev_map).fillna(df_test['clean_name'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ffdf7de",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------\n",
        "# Step 2: Cluster unmatched mentions with HDBSCAN\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "m_ids = df_test['mention_id'].values\n",
        "c_names = df_test['canonical_name'].values\n",
        "\n",
        "# cluster_labels_final[i] stores the HDBSCAN label for unmatched_indices[i]\n",
        "cluster_labels_final = np.full(len(unmatched_indices), -1, dtype=int)\n",
        "\n",
        "if unmatched_indices:\n",
        "    unmatched_embs = test_embs[unmatched_indices].astype('float32') \n",
        "    unmatched_types = df_test.iloc[unmatched_indices]['type'].fillna('UNKNOWN').values\n",
        "    unmatched_names = [c_names[idx] for idx in unmatched_indices] \n",
        "    \n",
        "    next_cluster_id = 0\n",
        "    unique_types = np.unique(unmatched_types)\n",
        "    \n",
        "    print(f\"Clustering {len(unmatched_indices)} mentions...\")\n",
        "    for ent_type in tqdm(unique_types, desc=\"HDBSCAN Blocks\"):\n",
        "        # Work block-wise per entity type\n",
        "        type_mask = (unmatched_types == ent_type)\n",
        "        type_indices = np.where(type_mask)[0]\n",
        "        \n",
        "        if len(type_indices) < 2: continue\n",
        "\n",
        "        # Optionally split very large blocks by first letter to speed up HDBSCAN\n",
        "        if len(type_indices) > 20000:\n",
        "            first_letters = np.array([str(unmatched_names[i])[0] if unmatched_names[i] else '#' for i in type_indices])\n",
        "            sub_units = np.unique(first_letters)\n",
        "        else:\n",
        "            sub_units = ['ALL']\n",
        "\n",
        "        for sub in sub_units:\n",
        "            if sub == 'ALL':\n",
        "                sub_idx = type_indices\n",
        "            else:\n",
        "                sub_mask = (first_letters == sub)\n",
        "                sub_idx = type_indices[sub_mask]\n",
        "\n",
        "            if len(sub_idx) < 2: continue\n",
        "\n",
        "            # Run HDBSCAN on this block of unmatched mentions\n",
        "            block_model = hdbscan.HDBSCAN(\n",
        "                min_cluster_size=2, \n",
        "                min_samples=1, \n",
        "                metric='euclidean', \n",
        "                cluster_selection_epsilon=0.5, \n",
        "                core_dist_n_jobs=-1,\n",
        "                prediction_data=False\n",
        "            )\n",
        "            block_labels = block_model.fit_predict(unmatched_embs[sub_idx])\n",
        "            \n",
        "            # Offset labels so that clusters from different blocks do not collide\n",
        "            for i, lbl in enumerate(block_labels):\n",
        "                if lbl != -1:\n",
        "                    cluster_labels_final[sub_idx[i]] = lbl + next_cluster_id\n",
        "            \n",
        "            if block_labels.max() != -1:\n",
        "                next_cluster_id += block_labels.max() + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d9a0c8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# -------------------------------------------------------------\n",
        "# Step 3: Merge KB matches and HDBSCAN clusters into final labels\n",
        "# -------------------------------------------------------------\n",
        "final_clusters = {}\n",
        "name_to_cluster_id = {}\n",
        "hdb_to_cluster_id = {}\n",
        "next_singleton_id = 0\n",
        "\n",
        "\n",
        "print(\"Finalizing clusters...\")\n",
        "# 1: KB Matches\n",
        "for i in tqdm(range(len(assignments)), desc=\"Pass 1: KB\"):\n",
        "    kb_idx = assignments[i]\n",
        "    if kb_idx != -1:\n",
        "        target_cluster = f\"kb_{kb_idx}\"\n",
        "        name = c_names[i]\n",
        "        final_clusters.setdefault(target_cluster, []).append(m_ids[i])\n",
        "        name_to_cluster_id[name] = target_cluster\n",
        "\n",
        "#2: Unmatched \n",
        "# use HDBSCAN labels and canonical names\n",
        "if unmatched_indices:\n",
        "    u_names = [c_names[idx] for idx in unmatched_indices]\n",
        "    u_ids = [m_ids[idx] for idx in unmatched_indices]\n",
        "\n",
        "    for i in tqdm(range(len(unmatched_indices)), desc=\"Pass 2: New\"):\n",
        "        label = cluster_labels_final[i]\n",
        "        name = u_names[i]\n",
        "        m_id = u_ids[i]\n",
        "        \n",
        "        # Reuse existing cluster if we have already seen this canonical name\n",
        "        if name in name_to_cluster_id:\n",
        "            target_cluster = name_to_cluster_id[name]\n",
        "        # Or reuse a cluster ID already assigned to this HDBSCAN label\n",
        "        elif label != -1 and label in hdb_to_cluster_id:\n",
        "            target_cluster = hdb_to_cluster_id[label]\n",
        "        # Otherwise create a new HDBSCAN-based cluster\n",
        "        elif label != -1:\n",
        "            target_cluster = f\"new_{label}\"\n",
        "            hdb_to_cluster_id[label] = target_cluster\n",
        "        # Finally, fall back to a singleton cluster for true outliers\n",
        "        else:\n",
        "            target_cluster = f\"singleton_{next_singleton_id}\"\n",
        "            next_singleton_id += 1\n",
        "        \n",
        "        name_to_cluster_id[name] = target_cluster\n",
        "        if label != -1: hdb_to_cluster_id[label] = target_cluster\n",
        "        final_clusters.setdefault(target_cluster, []).append(m_id)\n",
        "\n",
        "test_labels_output = list(final_clusters.values())\n",
        "with open(\"test_labels.json\", \"w\") as f:\n",
        "    json.dump(test_labels_output, f, indent=2)\n",
        "\n",
        "print(f\"Success! Total Clusters: {len(test_labels_output)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
